{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"This Kernel was inspired by https://www.kaggle.com/demonplus/fast-ai-starter-with-resnet-50\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nprint(os.listdir('../input'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import torch\nfrom tqdm import tqdm, tqdm_notebook\nimport random, torch, os, numpy as np\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss\nimport scipy as sp\n\ntorch.backends.cudnn.deterministic = True\n\n# def seed_everything(seed):\n#     random.seed(seed)\n#     os.environ['PYTHONHASHSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n\n\n# seed_everything(999)\ntorch.cuda.manual_seed(999)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from zipfile import ZipFile\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import PIL\nimport cv2\n\nbs = 96\n\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def crop_image1(img,tol=7):\n    # img is image data\n    # tol  is tolerance\n        \n    mask = img>tol\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n    #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img\n\ndef load_ben_color(path, sigmaX=10):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n        \n    return image\n\nIMG_SIZE = 512\n\ndef _load_format(path, convert_mode, after_open)->Image:\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = crop_image_from_gray(image)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0), 10) ,-4 ,128)\n                    \n    return Image(pil2tensor(image, np.float32).div_(255)) #return fastai Image format\n\nvision.data.open_image = _load_format","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # copy pretrained weights for resnet50 to the folder fastai will search by default\nPath('/tmp/.cache/torch/checkpoints/').mkdir(exist_ok=True, parents=True)\n# !cp '../input/resnet50/resnet50.pth' '/tmp/.cache/torch/checkpoints/resnet50-19c8e357.pth'\n#resnet50-19c8e357\n#resnet34-333f7ec4","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(os.listdir('../input/resized-2015-2019-blindness-detection-images/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_df():\n    base_image_dir = os.path.join('..', 'input/resized-2015-2019-blindness-detection-images/')\n    train_dir = os.path.join(base_image_dir,'resized train 15/')\n    df = pd.read_csv(os.path.join(base_image_dir, 'labels/trainLabels15.csv'))\n    df['path'] = df['image'].map(lambda x: os.path.join(train_dir,'{}.jpg'.format(x)))\n    df = df.drop(columns=['image'])\n    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n    test_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n    return df, test_df\n\ndf, test_df = get_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df.head())\ndf.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.rename(columns={\"level\": \"diagnosis\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"remove_n = 1000\ndrop_indices = np.random.choice(df[df.diagnosis==0].index, remove_n, replace=False)\ndf_sub = df.drop(drop_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create Stratified validation split (12.50%)\n#fastai does not include stratify option in train test data split, however according to the lecturer, \n#imbalance classifiers will be handle by the deep learning quite well, not sure this is true in this case\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=5, random_state=999)\ntr_ids, val_ids = next(cv.split(df_sub.path, df_sub.diagnosis))\nprint(len(tr_ids), len(val_ids))\n_ = df_sub.loc[val_ids].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print(val_ids)\n# print(tr_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# import zipfile\n# with zipfile.ZipFile('./train_images.zip', 'r') as zip_ref:\n#     zip_ref.extractall('./train_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# with zipfile.ZipFile('./test_images.zip', 'r') as zip_ref:\n#     zip_ref.extractall('./test_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tfms = ([\n#     RandTransform(tfm=TfmCrop (crop_pad), kwargs={'row_pct': (0.3, 1), 'col_pct': (0.1, 0.9),\\\n#                                                        'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True),\n#   RandTransform(tfm=TfmPixel (rgb_randomize), kwargs={'channel':0, 'thresh':0.1}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n#   RandTransform(tfm=TfmPixel (rgb_randomize), kwargs={'channel':2, 'thresh':0.1}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n  RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True),\n  RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-10.0, 10.0)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n  RandTransform(tfm=TfmAffine (zoom), kwargs={'scale': (1.0, 1.01), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n  RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.4, 0.6)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n  RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.8, 1.25)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True)],\n  [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#create data using fastai ImageDataBunch function, create from image list with lable.\n#simple data augmentation with flip and rotate since this is an eyeball image, the image is normalized using default imagenet_stats, another possible \n#option would be use the aptos19_stats, which not sure how to derive from yet\n# create image data bunch\n# \n# create image data bunch\n# data = ImageDataBunch.from_df('./', \n#                               df=df, \n#                               valid_pct=0.2,\n#                               folder=\"../input/diabetic-retinopathy-resized/resized_train\",\n#                               suffix=\".jpeg\",\n#                               ds_tfms=tfms,\n#                               size=224,\n#                               resize_method=ResizeMethod.SQUISH,\n#                               bs=96, \n#                               num_workers=0,\n#                              label_col='level', label_delim=',').normalize(imagenet_stats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# data.show_batch(rows=3, figsize=(7,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = (ImageList.from_df(df=df_sub,path='./', cols='path') \n        .split_by_idx(val_ids) \n        .label_from_df(cols='diagnosis',label_cls=FloatList) \n        .transform(tfms,size=224) \n        .databunch(bs=192,num_workers=16) \n        .normalize(imagenet_stats)  \n       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import torchvision.models as models\nwide_resnet50_2 = models.wide_resnet50_2(pretrained=False)\nmodel = nn.Sequential(\n#     nn.Conv2d(1, 8, 3, padding=1, bias=False),           # conv1\n#     *wresgroup(8, 8, k=2, N=6, dropout=0.3),             # conv2\n#     *wresgroup(16, 16, k=2, N=6, stride=2, dropout=0.3), # conv3\n#     *wresgroup(32, 32, k=2, N=6, stride=2, dropout=0.3), # conv4\n    wide_resnet50_2,\n    nn.BatchNorm1d(1000),\n    nn.ReLU(inplace=True),\n#     nn.AvgPool2d(1),                                     # the output of the last wresgroup is 7x7\n    Flatten(),\n    nn.Linear(1000, 1)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\nimport itertools as it\n#from torch.optim import Optimizer\n#credit - Lookahead implementation from LonePatient - https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n#credit2 - RAdam code by https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\n\n\nclass Ranger(Optimizer):\n    \n    def __init__(self, params, lr=1e-2, alpha=0.5, k=10, betas=(.9,0.999), eps=1e-8, weight_decay=1e-6):\n        #parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n        \n        #prep defaults and init torch.optim base\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super().__init__(params,defaults)\n        \n        #now we can get to work...\n        for group in self.param_groups:\n            group[\"step_counter\"] = 0\n            #print(\"group step counter init\")\n                      \n        #look ahead params\n        self.alpha = alpha\n        self.k = k \n        \n        #radam buffer for state\n        self.radam_buffer = [[None,None,None] for ind in range(10)]\n        \n        #lookahead weights\n        self.slow_weights = [[p.clone().detach() for p in group['params']]\n                                for group in self.param_groups]\n        \n        #don't use grad for lookahead weights\n        for w in it.chain(*self.slow_weights):\n            w.requires_grad = False\n        \n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n       \n        \n    def step(self, closure=None):\n        loss = None\n        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n        #Uncomment if you need to use the actual closure...\n        \n        #if closure is not None:\n            #loss = closure()\n            \n        #------------ radam\n        for group in self.param_groups:\n    \n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n    \n                p_data_fp32 = p.data.float()\n    \n                state = self.state[p]\n    \n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n    \n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n    \n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n    \n                state['step'] += 1\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n    \n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n    \n                if N_sma > 5:                    \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n    \n                p.data.copy_(p_data_fp32)\n        \n        \n        #---------------- end radam step\n        \n        #look ahead tracking and updating if latest batch = k\n        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n            group['step_counter'] += 1\n            if group['step_counter'] % self.k != 0:\n                continue\n            for p,q in zip(group['params'],slow_weights):\n                if p.grad is None:\n                    continue\n                q.data.add_(self.alpha,p.data - q.data)\n                p.data.copy_(q.data)\n            \n        \n            \n        return loss\n\noptar = partial(Ranger)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def quad_kappa(y_pred, y):\n    return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn = Learner(data, model, metrics=quad_kappa,\n#                     loss_func = MSELossFlat(),\n                    opt_func = optar,\n                    callback_fns=[\n                              partial(EarlyStoppingCallback, monitor='quad_kappa', min_delta=0.001, patience=2),\n                              partial(ReduceLROnPlateauCallback),\n                              partial(SaveModelCallback, every = 'improvement', monitor='quad_kappa', name='best2015')],\n                    model_dir=\"/tmp/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!ls ../input/wide-resnet-50-2-bottleneck-linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!cp '../input/wide-resnet-50-2-bottleneck-linear/wideresnet-50-2-bottleneck-linearhead.pth' '/tmp/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.load('wideresnet-50-2-bottleneck-linearhead');\nlearn.to_fp16()\nlearn.summary;","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lrs = learn.recorder.lrs\n# losses = learn.recorder.losses\n# mg = (np.gradient(np.array(losses))).argmin()\n# ml = np.argmin(losses)\n# min_grad_lr = lrs[mg]\n# print(min_grad_lr)\n# min_loss_lr0 = lrs[ml]/10\n# print(min_loss_lr0)\nmin_loss_lr0 = 3e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.fit_one_cycle(8,min_loss_lr0)\nlearn.load('best2015')\n# learn.destroy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nsample_df.head()\nlearn.data.add_test(ImageList.from_df(sample_df,'../input/aptos2019-blindness-detection',folder='test_images',suffix='.png'))\npreds1,y = learn.get_preds(DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold\n# cv = StratifiedKFold(n_splits=4, random_state=888)\n# tr_ids2, val_ids2 = next(cv.split(df_sub.path, df_sub.diagnosis))\n# print(len(tr_ids2), len(val_ids2))\n# _ = df_sub.loc[val_ids2].hist()\n# data = (ImageList.from_df(df=df_sub,path='./', cols='path') \n#         .split_by_idx(tr_ids) \n#         .label_from_df(cols='diagnosis',label_cls=FloatList) \n#         .transform(tfms,size=224) \n#         .databunch(bs=176,num_workers=16) \n#         .normalize(imagenet_stats)  \n#        )\n# learn.data = data\n# learn.to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.lr_find()\n# lrs = learn.recorder.lrs\n# losses = learn.recorder.losses\n# learn.recorder.plot(suggestion=True)\n# # mg = (np.gradient(np.array(losses))).argmin()\n# ml = np.argmin(losses)\n# # min_grad_lr = lrs[mg]\n# # print(min_grad_lr)\n# min_loss_lr = lrs[ml]/10\n# min_loss_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.fit_one_cycle(6,5e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.cuda.manual_seed(888)\nbase_image_dir2 = os.path.join('..', 'input/aptos2019-blindness-detection/')\ntrain_dir2 = os.path.join(base_image_dir2,'train_images/')\ndf2 = pd.read_csv(os.path.join(base_image_dir2, 'train.csv'))\ndf2['path'] = df2['id_code'].map(lambda x: os.path.join(train_dir2,'{}.png'.format(x)))\ndf2 = df2.drop(columns=['id_code'])\ndf2 = df2.sample(frac=1).reset_index(drop=True) #shuffle dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = (ImageList.from_df(df=df2,path='./',cols='path') \n        .split_by_rand_pct(0.2) \n        .label_from_df(cols='diagnosis',label_cls=FloatList) \n        .transform(tfms,size=224) \n        .databunch(bs=192,num_workers=16) \n        .normalize(imagenet_stats)  \n       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.destroy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = Learner(data, model, metrics=quad_kappa,\n#                     loss_func = MSELossFlat(),\n#                     opt_func = optar,\n                    callback_fns=[\n                              partial(EarlyStoppingCallback, monitor='quad_kappa', min_delta=0.001, patience=2),\n                              partial(ReduceLROnPlateauCallback),\n                              ShowGraph,\n                              partial(SaveModelCallback, every = 'improvement', monitor='quad_kappa', name='best2019')],\n                    model_dir=\"/tmp/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.load('best2015');\n# learn.data = data\nlearn.to_fp16()\n# learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # learn.recorder.plot(suggestion=True)\n# lrs = learn.recorder.lrs\n# losses = learn.recorder.losses\n# learn.recorder.plot(suggestion=True)\n# mg = (np.gradient(np.array(losses))).argmin()\n# ml = np.argmin(losses)\n# min_grad_lr = lrs[mg]\n# print(min_grad_lr)\n# min_loss_lr = lrs[ml]/10\n# min_loss_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# !cp '../input/newcrop/bestmodel3.pth' '/tmp/'\n# !ls /tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.save('stage-1');\n# learn.unfreeze()\n# learn.lr_find(start_lr = 1e-10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.fit_one_cycle(7, 1e-4)\n# learn.freeze_to(-3)\n# learn.fit_one_cycle(10,slice(1e-6,1e-4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.load('best2019');\n# valid_preds = learn.get_preds(ds_type=DatasetType.Valid)\n# train_preds = learn.get_preds(ds_type=DatasetType.Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n        print(-loss_partial(self.coef_['x']))\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\nsample_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.data.add_test(ImageList.from_df(sample_df,'../input/aptos2019-blindness-detection',folder='test_images',suffix='.png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# def run_subm(learn=learn, coefficients=[0.57, 1.57, 2.57, 3.57])\n#     opt = OptimizedRounder()\n#     preds,y = learn.get_preds(DatasetType.Test)\n#     tst_pred = opt.predict(preds, coefficients)\n#     sample_df.diagnosis = tst_pred.astype(int)\n#     sample_df.to_csv('submission.csv',index=False)\n#     print ('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"coefficients=[0.57, 1.57, 2.57, 3.57]\nopt = OptimizedRounder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds2,y = learn.get_preds(DatasetType.Test)\npreds = (0.7*preds1.numpy() + 0.3*preds2.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tst_pred = opt.predict(preds, coefficients)\nsample_df.diagnosis = tst_pred.astype(int)\nsample_df.to_csv('submission.csv',index=False)\nsample_df.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# run_subm()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mv {learn.model_dir}/*.pth .\nos.listdir()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}
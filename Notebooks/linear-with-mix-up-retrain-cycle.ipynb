{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from joblib import load, dump\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from torchvision import models as md\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "from functools import partial\n",
    "from torch.utils import model_zoo\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "#I could not figure out how to install package in local kernel so i just stole from github =)\n",
    "#code stolen from https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This file contains helper functions for building the model and for loading model parameters.\n",
    "These helper functions are built to mirror those in the official TensorFlow implementation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for the entire model (stem, all blocks, and head)\n",
    "GlobalParams = collections.namedtuple('GlobalParams', [\n",
    "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
    "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
    "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
    "\n",
    "\n",
    "# Parameters for an individual model block\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
    "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
    "\n",
    "\n",
    "# Change namedtuple defaults\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.width_coefficient\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_same_padding_conv2d(image_size=None):\n",
    "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
    "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
    "    if image_size is None:\n",
    "        return Conv2dDynamicSamePadding\n",
    "    else:\n",
    "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
    "\n",
    "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class Conv2dStaticSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "\n",
    "        # Calculate padding based on image size and save it\n",
    "        assert image_size is not None\n",
    "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "        else:\n",
    "            self.static_padding = Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.static_padding(x)\n",
    "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "########################################################################\n",
    "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def efficientnet_params(model_name):\n",
    "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
    "    params_dict = {\n",
    "        # Coefficients:   width,depth,res,dropout\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "    }\n",
    "    return params_dict[model_name]\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "\n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'(\\d.*)', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        # Check stride\n",
    "        assert (('s' in options and len(options['s']) == 1) or\n",
    "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
    "\n",
    "        return BlockArgs(\n",
    "            kernel_size=int(options['k']),\n",
    "            num_repeat=int(options['r']),\n",
    "            input_filters=int(options['i']),\n",
    "            output_filters=int(options['o']),\n",
    "            expand_ratio=int(options['e']),\n",
    "            id_skip=('noskip' not in block_string),\n",
    "            se_ratio=float(options['se']) if 'se' in options else None,\n",
    "            stride=[int(options['s'][0])])\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_block_string(block):\n",
    "        \"\"\"Encodes a block to a string.\"\"\"\n",
    "        args = [\n",
    "            'r%d' % block.num_repeat,\n",
    "            'k%d' % block.kernel_size,\n",
    "            's%d%d' % (block.strides[0], block.strides[1]),\n",
    "            'e%s' % block.expand_ratio,\n",
    "            'i%d' % block.input_filters,\n",
    "            'o%d' % block.output_filters\n",
    "        ]\n",
    "        if 0 < block.se_ratio <= 1:\n",
    "            args.append('se%s' % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append('noskip')\n",
    "        return '_'.join(args)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(string_list):\n",
    "        \"\"\"\n",
    "        Decodes a list of string notations to specify blocks inside the network.\n",
    "\n",
    "        :param string_list: a list of strings, each string is a notation of block\n",
    "        :return: a list of BlockArgs namedtuples of block args\n",
    "        \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(blocks_args):\n",
    "        \"\"\"\n",
    "        Encodes a list of BlockArgs to a list of strings.\n",
    "\n",
    "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
    "        :return: a list of strings, each string is a notation of block\n",
    "        \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
    "        return block_strings\n",
    "\n",
    "\n",
    "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
    "    \"\"\" Creates a efficientnet model. \"\"\"\n",
    "\n",
    "    blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "    blocks_args = BlockDecoder.decode(blocks_args)\n",
    "\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=drop_connect_rate,\n",
    "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "def get_model_params(model_name, override_params):\n",
    "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
    "    if model_name.startswith('efficientnet'):\n",
    "        w, d, s, p = efficientnet_params(model_name)\n",
    "        # note: all models have drop connect rate = 0.2\n",
    "        blocks_args, global_params = efficientnet(\n",
    "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
    "    else:\n",
    "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
    "    if override_params:\n",
    "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
    "        global_params = global_params._replace(**override_params)\n",
    "    return blocks_args, global_params\n",
    "\n",
    "\n",
    "url_map = {\n",
    "    'efficientnet-b0': 'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth',\n",
    "    'efficientnet-b1': 'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth',\n",
    "    'efficientnet-b2': 'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth',\n",
    "    'efficientnet-b3': 'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth',\n",
    "    'efficientnet-b4': 'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth',\n",
    "    'efficientnet-b5': 'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth',\n",
    "}\n",
    "\n",
    "def load_pretrained_weights(model, model_name, load_fc=True):\n",
    "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
    "    state_dict = model_zoo.load_url(url_map[model_name])\n",
    "    if load_fc:\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        state_dict.pop('_fc.weight')\n",
    "        state_dict.pop('_fc.bias')\n",
    "        res = model.load_state_dict(state_dict, strict=False)\n",
    "        assert str(res.missing_keys) == str(['_fc.weight', '_fc.bias']), 'issue loading pretrained weights'\n",
    "    print('Loaded pretrained weights for {}'.format(model_name))\n",
    "    \n",
    "    \n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "\n",
    "    Args:\n",
    "        block_args (namedtuple): BlockArgs, see above\n",
    "        global_params (namedtuple): GlobalParam, see above\n",
    "\n",
    "    Attributes:\n",
    "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params):\n",
    "        super().__init__()\n",
    "        self._block_args = block_args\n",
    "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
    "        self._bn_eps = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Expansion phase\n",
    "        inp = self._block_args.input_filters  # number of input channels\n",
    "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = self._block_args.kernel_size\n",
    "        s = self._block_args.stride\n",
    "        self._depthwise_conv = Conv2d(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
    "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = self._block_args.output_filters\n",
    "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "    def forward(self, inputs, drop_connect_rate=None):\n",
    "        \"\"\"\n",
    "        :param inputs: input tensor\n",
    "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
    "        :return: output of block\n",
    "        \"\"\"\n",
    "\n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self._block_args.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
    "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
    "\n",
    "    Args:\n",
    "        blocks_args (list): A list of BlockArgs to construct blocks\n",
    "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
    "\n",
    "    Example:\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__()\n",
    "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
    "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
    "        self._global_params = global_params\n",
    "        self._blocks_args = blocks_args\n",
    "\n",
    "        # Get static or dynamic convolution depending on image size\n",
    "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
    "        bn_eps = self._global_params.batch_norm_epsilon\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3  # rgb\n",
    "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self._blocks_args:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "            if block_args.num_repeat > 1:\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self._global_params)\n",
    "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = self._global_params.dropout_rate\n",
    "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "\n",
    "        # Head\n",
    "        x = relu_fn(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(inputs)\n",
    "\n",
    "        # Pooling and final linear layer\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self._dropout:\n",
    "            x = F.dropout(x, p=self._dropout, training=self.training)\n",
    "        x = self._fc(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, override_params=None):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
    "        return EfficientNet(blocks_args, global_params)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, num_classes=1000):\n",
    "        model = EfficientNet.from_name(model_name, override_params={'num_classes': num_classes})\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def get_image_size(cls, model_name):\n",
    "        cls._check_model_name_is_valid(model_name)\n",
    "        _, _, res, _ = efficientnet_params(model_name)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
    "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
    "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
    "        num_models = 4 if also_need_pretrained_weights else 8\n",
    "        valid_models = ['efficientnet_b'+str(i) for i in range(num_models)]\n",
    "        if model_name.replace('-','_') not in valid_models:\n",
    "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making model\n",
    "md_ef = EfficientNet.from_pretrained('efficientnet-b5', num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying weighst to the local directory \n",
    "!mkdir models\n",
    "!cp '../input/kaggle-public/abcdef.pth' 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    base_image_dir = os.path.join('..', 'input/aptos2019-blindness-detection/')\n",
    "    train_dir = os.path.join(base_image_dir,'train_images/')\n",
    "    df = pd.read_csv(os.path.join(base_image_dir, 'train.csv'))\n",
    "    df['path'] = df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x)))\n",
    "    df = df.drop(columns=['id_code'])\n",
    "    df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "    test_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "    return df, test_df\n",
    "\n",
    "df, test_df = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can play around with tfms and image sizes\n",
    "bs = 66\n",
    "sz = 224\n",
    "torch.manual_seed(1)\n",
    "tfms = ([RandTransform(tfm=TfmCrop (crop_pad), kwargs={'row_pct': (0.15, 0.85), 'col_pct': (0.1, 0.9), 'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmPixel (rgb_randomize), kwargs={'channel':2, 'thresh':0.6}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmPixel (rgb_randomize), kwargs={'channel':0, 'thresh':0.5}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmAffine (dihedral_affine), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-15.0, 15.0)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmAffine (zoom), kwargs={'scale': (1.0, 1.1), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.4, 0.6)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),\n",
    "  RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.8, 1.25)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True)],\n",
    " [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744 918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=4, random_state=42)\n",
    "tr_ids, val_ids = next(cv.split(df.path, df.diagnosis))\n",
    "print(len(tr_ids), len(val_ids))\n",
    "_ = df.iloc[val_ids].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_df(df=df,path='./',cols='path') \n",
    "        .split_by_idx(val_ids) \n",
    "        .label_from_df(cols='diagnosis',label_cls=FloatList) \n",
    "        .transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') \n",
    "        .databunch(bs=bs,num_workers=10) \n",
    "        .normalize(imagenet_stats)  \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qk(y_pred, y):\n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import itertools as it\n",
    "#from torch.optim import Optimizer\n",
    "#credit - Lookahead implementation from LonePatient - https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
    "#credit2 - RAdam code by https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\n",
    "\n",
    "\n",
    "class Ranger(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, betas=(.9,0.999), eps=1e-8, weight_decay=0):\n",
    "        #parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "        \n",
    "        #prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params,defaults)\n",
    "        \n",
    "        #now we can get to work...\n",
    "        for group in self.param_groups:\n",
    "            group[\"step_counter\"] = 0\n",
    "            #print(\"group step counter init\")\n",
    "                      \n",
    "        #look ahead params\n",
    "        self.alpha = alpha\n",
    "        self.k = k \n",
    "        \n",
    "        #radam buffer for state\n",
    "        self.radam_buffer = [[None,None,None] for ind in range(10)]\n",
    "        \n",
    "        #lookahead weights\n",
    "        self.slow_weights = [[p.clone().detach() for p in group['params']]\n",
    "                                for group in self.param_groups]\n",
    "        \n",
    "        #don't use grad for lookahead weights\n",
    "        for w in it.chain(*self.slow_weights):\n",
    "            w.requires_grad = False\n",
    "        \n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "       \n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        #note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n",
    "        #Uncomment if you need to use the actual closure...\n",
    "        \n",
    "        #if closure is not None:\n",
    "            #loss = closure()\n",
    "            \n",
    "        #------------ radam\n",
    "        for group in self.param_groups:\n",
    "    \n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "    \n",
    "                p_data_fp32 = p.data.float()\n",
    "    \n",
    "                state = self.state[p]\n",
    "    \n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "    \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "    \n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "    \n",
    "                state['step'] += 1\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "    \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "    \n",
    "                if N_sma > 5:                    \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "    \n",
    "                p.data.copy_(p_data_fp32)\n",
    "        \n",
    "        \n",
    "        #---------------- end radam step\n",
    "        \n",
    "        #look ahead tracking and updating if latest batch = k\n",
    "        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n",
    "            group['step_counter'] += 1\n",
    "            if group['step_counter'] % self.k != 0:\n",
    "                continue\n",
    "            for p,q in zip(group['params'],slow_weights):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                q.data.add_(self.alpha,p.data - q.data)\n",
    "                p.data.copy_(q.data)\n",
    "            \n",
    "        \n",
    "            \n",
    "        return loss\n",
    "\n",
    "optar = partial(Ranger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, \n",
    "                md_ef,\n",
    "                opt_func = optar,\n",
    "                metrics = [qk],\n",
    "                callback_fns = [\n",
    "                                partial(EarlyStoppingCallback, monitor='qk', min_delta=0.001, patience=2),\n",
    "                                partial(ReduceLROnPlateauCallback),\n",
    "#                               partial(GradientClipping, clip=0.2),\n",
    "                                partial(SaveModelCallback, every = 'improvement', monitor='qk', name='bestmodel')],\n",
    "                model_dir=\"models\").to_fp16()\n",
    "\n",
    "learn.data.add_test(ImageList.from_df(test_df,\n",
    "                                      '../input/aptos2019-blindness-detection',\n",
    "                                      folder='test_images',\n",
    "                                      suffix='.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n",
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 8.32E-04\n",
      "Min loss divided by 10: 1.00E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.load('abcdef');\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = learn.recorder.lrs\n",
    "losses = learn.recorder.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009999999999999998\n"
     ]
    }
   ],
   "source": [
    "mg = (np.gradient(np.array(losses))).argmin()\n",
    "ml = ml = np.argmin(losses)\n",
    "min_grad_lr = lrs[mg]\n",
    "min_loss_lr = lrs[ml]/10\n",
    "print(min_loss_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [4/6 32:06<16:03]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>qk</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.850979</td>\n",
       "      <td>0.351325</td>\n",
       "      <td>0.903736</td>\n",
       "      <td>07:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.534160</td>\n",
       "      <td>0.208587</td>\n",
       "      <td>0.920840</td>\n",
       "      <td>07:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.435558</td>\n",
       "      <td>0.226728</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>07:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368715</td>\n",
       "      <td>0.250678</td>\n",
       "      <td>0.901315</td>\n",
       "      <td>08:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 01:57<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with qk value: 0.9037356972694397.\n",
      "Better model found at epoch 1 with qk value: 0.9208395481109619.\n",
      "Epoch 2: reducing lr to 0.001615344966554843\n",
      "Epoch 3: reducing lr to 0.0009183775170928121\n",
      "Epoch 4: early stopping\n",
      "Epoch 4: reducing lr to 0.0002646067752237381\n",
      "set state called\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(6,min_loss_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n",
      "set state called\n",
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.31E-07\n",
      "Min loss divided by 10: 9.12E-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = learn.recorder.lrs\n",
    "losses = learn.recorder.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0892961308540395e-08\n"
     ]
    }
   ],
   "source": [
    "mg = (np.gradient(np.array(losses))).argmin()\n",
    "ml = ml = np.argmin(losses)\n",
    "min_grad_lr = lrs[mg]\n",
    "min_loss_lr = lrs[ml]/10\n",
    "print(min_loss_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.00% [3/10 24:45<57:45]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>qk</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.346866</td>\n",
       "      <td>0.215098</td>\n",
       "      <td>0.919018</td>\n",
       "      <td>07:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353904</td>\n",
       "      <td>0.225491</td>\n",
       "      <td>0.914683</td>\n",
       "      <td>08:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364403</td>\n",
       "      <td>0.236217</td>\n",
       "      <td>0.911054</td>\n",
       "      <td>08:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 02:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with qk value: 0.9190184473991394.\n",
      "Epoch 1: reducing lr to 3.1757301188981395e-09\n",
      "Epoch 2: reducing lr to 4.1785922617080794e-09\n",
      "Epoch 3: early stopping\n",
      "Epoch 3: reducing lr to 3.971687728203952e-09\n",
      "set state called\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, max_lr = min_loss_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n",
      "set state called\n",
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.load('bestmodel')\n",
    "torch.manual_seed(2)\n",
    "data = (ImageList.from_df(df=df,path='./',cols='path') \n",
    "        .split_by_idx(val_ids) \n",
    "        .label_from_df(cols='diagnosis',label_cls=FloatList) \n",
    "        .transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') \n",
    "        .databunch(bs=bs,num_workers=10) \n",
    "        .normalize(imagenet_stats)  \n",
    "       )\n",
    "learn.data = data\n",
    "learn.to_fp16()\n",
    "learn.mixup()\n",
    "learn.lr_find()\n",
    "# learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = learn.recorder.lrs\n",
    "losses = learn.recorder.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007585775750291836\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      66.67% [4/6 33:40<16:50]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>qk</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.738557</td>\n",
       "      <td>0.227735</td>\n",
       "      <td>0.914164</td>\n",
       "      <td>08:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.707291</td>\n",
       "      <td>0.213435</td>\n",
       "      <td>0.917188</td>\n",
       "      <td>08:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.205541</td>\n",
       "      <td>0.916244</td>\n",
       "      <td>08:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.723959</td>\n",
       "      <td>0.212227</td>\n",
       "      <td>0.914533</td>\n",
       "      <td>07:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='14' class='' max='14', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [14/14 01:50<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with qk value: 0.9141644835472107.\n",
      "Better model found at epoch 1 with qk value: 0.9171875715255737.\n",
      "Epoch 3: reducing lr to 6.96660589877588e-05\n",
      "Epoch 4: early stopping\n",
      "Epoch 4: reducing lr to 2.0072476588551556e-05\n",
      "set state called\n"
     ]
    }
   ],
   "source": [
    "mg = (np.gradient(np.array(losses))).argmin()\n",
    "ml = ml = np.argmin(losses)\n",
    "min_grad_lr = lrs[mg]\n",
    "min_loss_lr = lrs[ml]/10\n",
    "print(min_loss_lr)\n",
    "learn.fit_one_cycle(6,min_loss_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/abhishek/optimizer-for-quadratic-weighted-kappa\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        print(-loss_partial(self.coef_['x']))\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('bestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e892aab366b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptcoef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_subm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-e892aab366b6>\u001b[0m in \u001b[0;36mrun_subm\u001b[0;34m(learn, coefficients)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_subm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedRounder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtst_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnosis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_type, activ, with_loss, n_batch, pbar)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         return get_preds(self.model, self.dl(ds_type), cb_handler=CallbackHandler(self.callbacks),\n\u001b[0;32m--> 345\u001b[0;31m                          activ=activ, loss_func=lf, n_batch=n_batch, pbar=pbar)\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, dl, pbar, cb_handler, activ, loss_func, n_batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     res = [torch.cat(o).cpu() for o in\n\u001b[0;32m---> 44\u001b[0;31m            zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mNoneReduceOnCPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mset_dl\u001b[0;34m(self, dl)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;34m\"Set the current `dl` used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cb_dl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "def run_subm(learn=learn, coefficients=[0.5, 1.5, 2.5, 3.5]):\n",
    "    opt = OptimizedRounder()\n",
    "    preds,y = learn.get_preds(DatasetType.Test)\n",
    "    tst_pred = opt.predict(preds, coefficients)\n",
    "    test_df.diagnosis = tst_pred.astype(int)\n",
    "    optcoef = opt.coefficients\n",
    "    return preds, optcoef\n",
    "\n",
    "run_subm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-775d78603241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtst_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnosis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_type, activ, with_loss, n_batch, pbar)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         return get_preds(self.model, self.dl(ds_type), cb_handler=CallbackHandler(self.callbacks),\n\u001b[0;32m--> 345\u001b[0;31m                          activ=activ, loss_func=lf, n_batch=n_batch, pbar=pbar)\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, dl, pbar, cb_handler, activ, loss_func, n_batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     res = [torch.cat(o).cpu() for o in\n\u001b[0;32m---> 44\u001b[0;31m            zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mNoneReduceOnCPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mset_dl\u001b[0;34m(self, dl)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;34m\"Set the current `dl` used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cb_dl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "preds,y = learn.get_preds(DatasetType.Test)\n",
    "tst_pred = opt.predict(preds, coefficients)\n",
    "test_df.diagnosis = tst_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  0005cfc8afb6          0\n",
       "1  003f0afdcd15          0\n",
       "2  006efc72b638          0\n",
       "3  00836aaacf06          0\n",
       "4  009245722fa4          0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1928\n",
       "Name: mdiagnosis, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = [0.7, 0.3]\n",
    "sample_df['mdiagnosis'] = 1*test_df.diagnosis\n",
    "sample_df.mdiagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1928\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = [0.5,1.5,2.5,3.5]\n",
    "# sample_df.fdiagnosis = opt.predict(sample_df.fdiagnosis, coef)\n",
    "submission = pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n",
    "submission.diagnosis = sample_df.mdiagnosis.astype(int)\n",
    "submission.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdef.pth',\n",
       " 'bestmodel.pth',\n",
       " 'tmp.pth',\n",
       " '__notebook__.ipynb',\n",
       " 'submission.csv',\n",
       " '__output__.json',\n",
       " 'models']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mv {learn.model_dir}/*.pth .\n",
    "os.listdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
